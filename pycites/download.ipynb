{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "from appdirs import user_cache_dir, user_data_dir\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# currently unused:\n",
    "# import janitor\n",
    "# import pyarrow\n",
    "# from pyarrow import csv\n",
    "# import pyarrow.feather as feather\n",
    "# import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_filename():\n",
    "    with requests.head(cites_url) as r:\n",
    "        zip_file = (\n",
    "            r.headers.get(\"Content-Disposition\", default_filename)\n",
    "            .split(\"filename=\")[-1]\n",
    "            .strip('\"')\n",
    "        )  # has weird header, which contains filename\n",
    "    return zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum(filename, hash_factory=hashlib.md5, chunk_num_blocks=128):\n",
    "    \"\"\" https://stackoverflow.com/a/4213255 \"\"\"\n",
    "    h = hash_factory()\n",
    "    with open(filename, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_num_blocks * h.block_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def download_cites_trade_zip(zip_file):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/a/39217788\n",
    "    https://stackoverflow.com/a/37573701\n",
    "    \"\"\"\n",
    "    cachedir = zip_file.parent\n",
    "    cachedir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\n",
    "        f\"Downloading CITES Trade database zip file from {cites_url} to {zip_file}...\"\n",
    "    )\n",
    "    with requests.get(cites_url, stream=True) as r:\n",
    "        local_filename = cachedir / zip_file\n",
    "        content_length = int(r.headers.get(\"content-length\", 0))\n",
    "        total_size = content_length if content_length > 0 else None\n",
    "        block_size = 1024\n",
    "        with tqdm(total=total_size, unit=\"B\", unit_scale=True, unit_divisor=1024) as t:\n",
    "            with open(local_filename, \"wb\") as f:\n",
    "                for data in r.iter_content(block_size):\n",
    "                    t.update(len(data))\n",
    "                    f.write(data)\n",
    "    #                     shutil.copyfileobj(r.raw, f)\n",
    "    #                     t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(zip_file, cleanup=False):\n",
    "    dest = zip_file.parent  # TODO: make a subdir to contain this?\n",
    "    print(f\"Extracting CITES Trade database zip file {zip_file} to {dest}...\")\n",
    "    # dest.mkdir(parents=True, exist_ok=True)  # no longer necessary because already exists\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_file:\n",
    "        for file in tqdm(\n",
    "            iterable=zip_file.namelist(), total=len(zip_file.namelist()), unit=\"files\"\n",
    "        ):\n",
    "            zip_file.extract(member=file, path=dest)\n",
    "    if cleanup:\n",
    "        zip_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(csv_dir, cleanup=False):\n",
    "    datasets = []\n",
    "    files = list(csv_dir.glob(\"*.csv\"))\n",
    "    print(f\"Reading in CITES Trade database CSV files from {csv_dir}...\")\n",
    "    for f in tqdm(iterable=files, total=len(files), unit=\"files\"):\n",
    "        d = pd.read_csv(f, low_memory=False)\n",
    "        # make sure Year and Quantiy values are numeric (int and float, respectively)\n",
    "        # drop rows that are missing Year\n",
    "        # ensure Year is between 1970 and 2020\n",
    "        d = d[pd.to_numeric(d[\"Year\"], errors=\"coerce\").notnull()]\n",
    "        d = d.dropna(subset=[\"Year\"])\n",
    "        d[\"Year\"] = d[\"Year\"].astype(int)\n",
    "        d[\"Quantity\"] = pd.to_numeric(d[\"Quantity\"], errors=\"coerce\")\n",
    "        d[\"Quantity\"] = d[\"Quantity\"].astype(float)\n",
    "        d = d[d[\"Year\"] > 1970]\n",
    "        d = d[d[\"Year\"] < 2020]\n",
    "        datasets.append(d)\n",
    "    print(\"Combining CSV files into a single DataFrame...\")\n",
    "    df = pd.concat(datasets)\n",
    "    print(\"Sorting and re-indexing DataFrame...\")\n",
    "    df = df.sort_values(\n",
    "        by=[\n",
    "            \"Year\",\n",
    "            \"Taxon\",\n",
    "            \"Order\",\n",
    "            \"Family\",\n",
    "            \"Genus\",\n",
    "            \"Term\",\n",
    "            \"Importer\",\n",
    "            \"Exporter\",\n",
    "            \"Appendix\",\n",
    "        ]\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "    if cleanup:\n",
    "        for f in files:\n",
    "            f.unlink()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default locations\n",
    "cites_url = \"https://trade.cites.org/cites_trade/download_db\"\n",
    "default_filename = \"trade_database.zip\"\n",
    "\n",
    "cachedir = Path(user_cache_dir(\"pycites\", \"ltirrell\"))\n",
    "datadir = Path(user_data_dir(\"pycites\", \"ltirrell\"))\n",
    "zip_file = cachedir / get_zip_filename()\n",
    "csv_file = datadir / zip_file.name.replace(\"zip\", \"csv.gz\")\n",
    "\n",
    "known_zip_md5 = \"3d774b109c3ebf3594cf0fd4c20c1d1b\"\n",
    "known_csv_md5 = \"920614bc5a9219849b1626a653d5ea64\"\n",
    "\n",
    "# column dtypes (not currently used)\n",
    "column_types = {\n",
    "    \"Year\": \"int\",\n",
    "    \"Appendix\": \"string\",\n",
    "    \"Taxon\": \"string\",\n",
    "    \"Class\": \"string\",\n",
    "    \"Order\": \"string\",\n",
    "    \"Family\": \"string\",\n",
    "    \"Genus\": \"string\",\n",
    "    \"Term\": \"string\",\n",
    "    \"Quantity\": \"float\",\n",
    "    \"Unit\": \"string\",\n",
    "    \"Importer\": \"string\",\n",
    "    \"Exporter\": \"string\",\n",
    "    \"Origin\": \"string\",\n",
    "    \"Purpose\": \"string\",\n",
    "    \"Source\": \"string\",\n",
    "    \"Reporter.type\": \"string\",\n",
    "    \"Import.permit.RandomID\": \"string\",\n",
    "    \"Export.permit.RandomID\": \"string\",\n",
    "    \"Origin.permit.RandomID\": \"string\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def update_data(zip_file=zip_file, csv_file=csv_file, force_update=True, cleanup=False):\n",
    "    # TODO: cleanup\n",
    "    if not zip_file.exists() or force_update:\n",
    "        download_cites_trade_zip(zip_file)\n",
    "        zip_md5 = checksum(zip_file)\n",
    "        if zip_md5 != known_zip_md5:\n",
    "            raise ValueError(\n",
    "                f\"md5 sum of zip file ({zip_md5}) does not match known value: {known_zip_md5}\"\n",
    "            )\n",
    "    extract_files(zip_file, csv_file.parent, cleanup=cleanup)\n",
    "    df = combine_csv(zip_file.parent, cleanup)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def load_data(csv_file=csv_file, update=False, update_kwargs=None):\n",
    "    if update:\n",
    "        if update_kwargs is not None:\n",
    "            update_data(**update_kwargs)\n",
    "        else:\n",
    "            update_data()\n",
    "\n",
    "    if not csv_file.exists():\n",
    "        raise FileNotFoundError(f\"The file {csv_file} does not exist!\")\n",
    "\n",
    "    csv_md5 = checksum(csv_file)\n",
    "    if csv_md5 != known_csv_md5:\n",
    "        raise ValueError(\n",
    "            f\"md5 sum of zip file ({csv_md5}) does not match known value: {known_csv_md5}\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('pycites': conda)",
   "language": "python",
   "name": "python38364bitpycitesconda301c197b92604ee8a3a0bec38518af4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
